<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Manas | Robotics Perception, Cognition & Controls Algorithms</title>
  <meta name="description" content="Portfolio of Manas Dixit — robotics, state estimation, SLAM, Controls and perception projects." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    :root{ --bg:#f5f7fb; --card:#ffffff; --ink:#0f172a; --muted:#475569; --line:#e2e8f0; --brand:#2563eb; --brand2:#14b8a6; --radius:14px; --shadow:0 6px 18px rgba(2,6,23,.08); }
    *{box-sizing:border-box}
    html,body{height:100%}
    html{scroll-behavior:smooth}
    body{margin:0;font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;background:var(--bg);color:var(--ink);line-height:1.6}
    a{color:var(--brand);text-decoration:none}
    a:hover{text-decoration:underline}
    .container{max-width:1080px;margin:0 auto;padding:0 20px}

    /* NAV */
    .nav{position:sticky;top:0;background:rgba(255,255,255,.9);backdrop-filter:blur(8px);border-bottom:1px solid var(--line);z-index:50}
    .nav-inner{display:flex;align-items:center;justify-content:space-between;padding:14px 0}
    .brand{font-weight:800;letter-spacing:.2px}
    .nav-links{display:flex;gap:12px;flex-wrap:wrap}
    .btn{display:inline-flex;align-items:center;gap:8px;border:1px solid var(--line);background:#fff;color:var(--ink);padding:8px 12px;border-radius:999px;box-shadow:var(--shadow);cursor:pointer}
    .btn:hover{background:#eef2ff}

    /* HERO */
    .hero{padding:56px 0}
    .hero h1{font-size:clamp(34px,5vw,60px);line-height:1.1;margin:0 0 10px}
    .hero p{color:var(--muted);margin:8px 0 18px}
    .chips{margin-top:20px}
    .chip-group{margin-bottom:20px}
    .chip-group h3{margin:0 0 8px;font-size:16px;font-weight:600;color:var(--ink)}
    .chip-list{display:flex;flex-wrap:wrap;gap:8px}
    .chip{font-size:12px;color:#0f172a;background:#e0e7ff;border:1px solid #c7d2fe;padding:6px 9px;border-radius:999px}

    /* SECTIONS */
    section{padding:36px 0}
    .title{font-size:26px;margin:0 0 10px}
    .subtitle{color:var(--muted);margin:-2px 0 18px}

    /* PROJECTS GRID */
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:18px}
    .card{background:var(--card);border:1px solid var(--line);border-radius:var(--radius);overflow:hidden;box-shadow:var(--shadow)}
    .thumb{width:100%;aspect-ratio:16/9;object-fit:cover;border-bottom:1px solid var(--line)}
    .card-body{padding:14px 14px 18px}
    .tags{margin-top:6px}
    .tag{font-size:12px;color:#334155;background:#f1f5f9;border:1px solid #e2e8f0;padding:4px 8px;border-radius:999px;margin-right:6px}
    .row-end{display:flex;justify-content:flex-end;margin-top:12px}

    /* FIX 1: Make ONLY selected cards narrower */
    .card.narrow{max-width:380px}

    /* FIX 2: Make thumbnail naturally shorter (scales with width, no weird cropping) */
    .thumb.sm{aspect-ratio:16/8}

    /* FIX 3: Description font size matches subheading size */
    .card-body p{margin:8px 0 18px;font-size:15px;line-height:1.5;color:var(--muted)}
    .card-body h3{font-size:18px;line-height:1.25;font-weight:600;margin:0 0 6px;}

    /* PROJECT CATEGORY HEADERS */
    .proj-block{margin-top:18px}
    .proj-h{display:flex;align-items:baseline;justify-content:space-between;gap:12px;margin:22px 0 10px}
    .proj-h h3{margin:0;font-size:18px}
    .proj-h p{margin:0;color:var(--muted);font-size:14px}
    .proj-sub{margin:14px 0 10px}
    .proj-sub h4{margin:0 0 10px;font-size:15px;color:var(--ink);font-weight:650}
    .divider{height:1px;background:var(--line);margin:18px 0}

    /* DETAIL PAGE */
    main.hidden{display:none}
    .detail-head{display:flex;flex-wrap:wrap;align-items:flex-end;justify-content:space-between;gap:16px;margin:6px 0 18px}
    .crumbs{color:var(--muted)}
    .gallery{display:grid;grid-template-columns:repeat(auto-fit,minmax(320px,1fr));gap:16px}
    .vid{background:var(--card);border:1px solid var(--line);border-radius:var(--radius);overflow:hidden}
    .vid iframe{width:100%;aspect-ratio:16/9;border:0;display:block}
    .caption{padding:10px 12px;font-size:14px;color:var(--muted);border-top:1px solid var(--line)}

    /* ABOUT & CONTACT */
    .about{display:grid;grid-template-columns:1.2fr .8fr;gap:18px}
    @media (max-width:900px){.about{grid-template-columns:1fr}}
    footer{border-top:1px solid var(--line);color:var(--muted);padding:16px 0;margin-top:32px}

    /* SKILLS */
    .skills-logos{display:grid;grid-template-columns:repeat(auto-fit,minmax(140px,1fr));gap:14px}
    .tool{display:flex;align-items:center;gap:10px;background:var(--card);border:1px solid var(--line);border-radius:12px;padding:10px}
    .tool img{width:22px;height:22px}
    .tool span{font-size:14px}
    .pill{display:inline-flex;align-items:center;border:1px dashed var(--line);padding:8px 10px;border-radius:999px;color:var(--muted)}

    /* ADDED: side-by-side media layout for mono-3d write-up */
    .media-row{display:grid;grid-template-columns:1fr 1fr;gap:12px;margin:10px 0 14px}
    .media{background:#fff;border:1px solid var(--line);border-radius:12px;overflow:hidden}
    .media img{width:100%;height:auto;display:block}
    .media iframe{width:100%;aspect-ratio:16/9;border:0;display:block}
    @media (max-width:900px){.media-row{grid-template-columns:1fr}}
  </style>
</head>
<body>
  <nav class="nav">
    <div class="container nav-inner">
      <div class="brand">Manas | Robotics Perception, Cognition & Controls Algorithms</div>
      <div class="nav-links">
        <a class="btn" href="#about">About</a>
        <a class="btn" href="#projects">Projects</a>
        <a class="btn" href="#skills">Skills</a>
        <a class="btn" href="#publications">Publications</a>
        <a class="btn" href="#contact">Contact</a>
        <a class="btn" href="Manas_dixit_resume_full_time_20250915.pdf" download="Manas_Dixit_Resume.pdf">Resume</a>
      </div>
    </div>
  </nav>

  <main id="home" class="container">
    <section class="hero">
      <h1>Hi, I'm Manas — building reliable robot perception, cognition and control algorithms.</h1>
      <p>MS Robotics @ UMN · Algorithms Development Intern @ Magna (ADAS) · ex-SEDEMAC Controls. Building end-to-end robotics software: SLAM in dynamic scenes, EKF/UKF pipelines, RL-tuned control, and ROS 2/C++/Python tooling for embedded and autonomous systems.</p>

      <div class="chips">
        <div class="chip-group">
          <h3>Estimation & Controls</h3>
          <div class="chip-list">
            <span class="chip">EKF / UKF</span>
            <span class="chip">Bayesian Filtering</span>
            <span class="chip">PID / Optimal Control</span>
            <span class="chip">Sensor Fusion</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Perception & AI</h3>
          <div class="chip-list">
            <span class="chip">Visual SLAM</span>
            <span class="chip">YOLOv8</span>
            <span class="chip">SegFormer</span>
            <span class="chip">3D Scene Reconstruction</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Robotics Software</h3>
          <div class="chip-list">
            <span class="chip">ROS 2</span>
            <span class="chip">Gazebo • RViz</span>
            <span class="chip">Inverse Kinematics</span>
            <span class="chip">Teleoperation</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Programming & Tools</h3>
          <div class="chip-list">
            <span class="chip">C++ • Python</span>
            <span class="chip">PyTorch • TensorFlow</span>
            <span class="chip">Docker • WSL2</span>
            <span class="chip">Git • JIRA</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Hardware & Sensors</h3>
          <div class="chip-list">
            <span class="chip">Radar • Camera • LiDAR</span>
            <span class="chip">IMU • Wheel Encoders</span>
            <span class="chip">Kinova Gen3</span>
            <span class="chip">TurtleBot3</span>
          </div>
        </div>
      </div>

    </section>

    <!-- ABOUT before projects -->
    <section id="about">
      <h2 class="title">About</h2>
      <div class="about">
        <div>
          <p>I’m a Robotics MS student working across estimation, perception, and control. Projects span CTRA/CA-based EKF pipelines (Magna ADAS), dynamic visual SLAM with semantic filtering, and ROS 2 navigation + IK for an agricultural rover with a 3-DOF arm.</p>
          <p>I enjoy building robotics systems end-to-end, transforming concepts and simulations into reliable pipelines validated in real-world environments.</p>
        </div>
        <div>
          <div class="chips"><span class="chip">Open to 2026 roles</span><span class="chip">US-based</span></div>
        </div>
      </div>
    </section>

    <!-- PROJECTS (structured) -->
    <section id="projects">
      <h2 class="title">Projects</h2>
      <p class="subtitle">Grouped by domain to make scanning easier.</p>

      <!-- 1) AUTONOMY -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Autonomy</h3>
          <p>SLAM • Sensor Fusion • Navigation</p>
        </div>

        <div class="proj-sub">
          <h4>SLAM</h4>
          <div class="grid">
            <article class="card narrow">
              <img class="thumb sm" src="https://img.youtube.com/vi/uFI8JMBfABI/0.jpg" alt="Dynamic SLAM"/>
              <div class="card-body">
                <h3>Dynamic Visual SLAM with SegFormer</h3>
                <p>ORB-SLAM3 + SegFormer masks for robust tracking in dynamic environments.</p>
                <div class="tags"><span class="tag">VIO/SLAM</span><span class="tag">SegFormer</span><span class="tag">C++</span></div>
                <div class="row-end"><button class="btn details" data-project="dynamic-slam">→ details</button></div>
              </div>
            </article>
          </div>
        </div>

        <div class="proj-sub">
          <h4>Sensor Fusion</h4>
          <div class="grid">
            <article class="card narrow">
              <img class="thumb sm" src="https://img.youtube.com/vi/2EUyFGxl5i8/0.jpg" alt="CTRA vs CA"/>
              <div class="card-body">
                <h3>CTRA Motion Model in EKF Sensor Fusion</h3>
                <p>Sharper maneuver handling than CA; tuned Q/R and Jacobians in a ROS 2 fusion pipeline.</p>
                <div class="tags"><span class="tag">EKF</span><span class="tag">CTRA</span><span class="tag">ROS 2</span></div>
                <div class="row-end"><button class="btn details" data-project="ctra-ekf">→ details</button></div>
              </div>
            </article>
          </div>
        </div>

        <div class="proj-sub">
          <h4>Navigation</h4>
          <div class="grid">
            <!-- Thesis project card under Navigation -->
            <article class="card narrow">
              <img class="thumb sm" src="https://img.youtube.com/vi/JlAoLlOukuA/hqdefault.jpg" alt="Thesis Project Navigation"/>
              <div class="card-body">
                <h3>Thesis: Learning-Based Navigation in Unstructured Cornfields</h3>
                <p>Mapping + Nav2 autonomy in Isaac Sim, then expert teleop data collection to train an imitation-learning local planner for unstructured cornfields.</p>
                <div class="tags">
                  <span class="tag">Isaac Sim</span>
                  <span class="tag">ROS 2</span>
                  <span class="tag">Nav2</span>
                  <span class="tag">SLAM Toolbox</span>
                  <span class="tag">Imitation Learning</span>
                </div>
                <div class="row-end"><button class="btn details" data-project="thesis-cornfield-nav">→ details</button></div>
              </div>
            </article>
          </div>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 2) PERCEPTION -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Perception</h3>
          <p>Vision-based control and sensing pipelines</p>
        </div>

        <div class="grid">
          <article class="card narrow">
            <img class="thumb sm" src="https://img.youtube.com/vi/02knEVV_Q8w/0.jpg" alt="IBVS"/>
            <div class="card-body">
              <h3>IBVS for TurtleBot3</h3>
              <p>Image-space control loop with interaction matrix and closed-loop regulation.</p>
              <div class="tags"><span class="tag">Control</span><span class="tag">ROS 2</span></div>
              <div class="row-end"><button class="btn details" data-project="ibvs-tb3">→ details</button></div>
            </div>
          </article>

          <!-- 2D->3D Single Image Reconstruction project card -->
          <article class="card narrow">
            <img class="thumb sm" src="https://img.youtube.com/vi/iy5rMMb7dKg/hqdefault.jpg" alt="2D to 3D Reconstruction"/>
            <div class="card-body">
              <h3>3D Reconstruction from a Single Image</h3>
              <p>Fine-tuned GLPN for monocular depth on KITTI and back-projected depth into a colored 3D point cloud.</p>
              <div class="tags"><span class="tag">Monocular Depth</span><span class="tag">GLPN</span><span class="tag">3D</span><span class="tag">PyTorch</span></div>
              <div class="row-end"><button class="btn details" data-project="mono-3d-recon">→ details</button></div>
            </div>
          </article>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 3) AI -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>AI</h3>
          <p>Deep learning • Reinforcement learning</p>
        </div>

        <div class="grid">
          <article class="card">
            <img class="thumb" src="https://dummyimage.com/800x450/0d152b/cfe0ff&text=YOLOv4+Training" alt="YOLOv4"/>
            <div class="card-body">
              <h3>YOLOv4 Adversarial Training</h3>
              <p>Custom PyTorch pipeline for robust detection via adversarial augmentation.</p>
              <div class="tags"><span class="tag">PyTorch</span><span class="tag">Detection</span></div>
              <div class="row-end"><button class="btn details" data-project="yolov4-adv">→ details</button></div>
            </div>
          </article>

          <article class="card">
            <img class="thumb" src="https://img.youtube.com/vi/vhEqvj8y0g4/0.jpg" alt="Kinova Gen3 RL Control"/>
            <div class="card-body">
              <h3>RL Control Environment for Kinova Gen3 Lite</h3>
              <p>Joint-space continuous control environment with curriculum and physics-grounded reward shaping.</p>
              <div class="tags"><span class="tag">Reinforcement Learning</span><span class="tag">Kinova Gen3</span><span class="tag">PyBullet</span></div>
              <div class="row-end"><button class="btn details" data-project="kinova-rl">→ details</button></div>
            </div>
          </article>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 4) CONTROLS -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Controls</h3>
          <p>Coming soon</p>
        </div>

        <div class="grid">
          <!-- intentionally blank -->
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="publications">
      <h2 class="title">Publications</h2>
      <div class="card" style="padding:16px">
        <h3 style="margin:0 0 8px;font-size:18px;line-height:1.25;font-weight:600;">Modelling of a Transient VCRS and Virtual Optimal Charge Determination for Automobiles</h3>
        <p style="margin:0 0 12px;font-size:15px;line-height:1.5;color:var(--muted)">
          Experimental research on the Vapor Compression Refrigeration System (VCRS) used in automobiles and other air conditioning systems is abundant in literature but it comes with inherent problems like the cost of the setup, time consumption, and tediousness of the process. Therefore, the objective of this study is to develop the fully-fledged transient or dynamic model of R134a based VCRS and further utilize it to virtually determine the optimal refrigerant charge quantity. The Simulink model developed in this work is an integration of physics-based mathematical models for each component (compressor, heat exchangers, and expansion valve), working together in a cycle where the compressor dictates the transient process. A sample simulation has been performed by taking 5 bar as the initial pressure of suction and discharge tanks and the steady state results (Discharge pressure, suction pressure, refrigerant mass flow rate, superheat and subcool temperatures) obtained. The results are found to closely follow the similar trends as the experimental results, hence validating the correctness of the Simulink model. Further, Using the proposed transient model, simulations have been performed for different initial state conditions and steady state plots (discharge pressure V/s charge, cooling capacity V/s charge, COP V/s charge, Subcool and Superheat V/s charge) have been constructed. Analysis of these plots shows that the values of cooling capacity and COP attain maxima simultaneously corresponding to 650 grams of charge. Hence, using maximum COP method for optimal charge determination, the optimal charge quantity has been calculated to be around 650 grams. In the end, this work hopes to provide researchers a transient model that can be used as a cost-effective substitute of the experimental process.
        </p>
        <div class="row-end">
          <a class="btn" target="_blank" rel="noreferrer" href="https://www.sae.org/papers/modelling-a-transient-vcrs-virtual-optimal-charge-determination-automobiles-2021-28-0255">Read paper</a>
        </div>
      </div>
    </section>

    <!-- SKILLS -->
    <section id="skills">
      <h2 class="title">Skills — Tools & Software</h2>
      <div class="skills-logos" id="skillsGrid"></div>
    </section>

    <!-- CONTACT -->
    <section id="contact">
      <h2 class="title">Contact</h2>
      <div class="card" style="padding:16px">
        <p>Email: <a href="mailto:manasdixit13@gmail.com">manasdixit13@gmail.com</a> · LinkedIn: <a target="_blank" rel="noreferrer" href="https://www.linkedin.com/in/manas2/">/in/manas2</a> · GitHub: <a target="_blank" rel="noreferrer" href="https://github.com/Manasdixit02">@Manasdixit02</a></p>
      </div>
    </section>

    <footer>© <span id="year"></span> Manas Dixit</footer>
  </main>

  <!-- DETAIL (Project page) -->
  <main id="detail" class="container hidden">
    <section class="detail-head">
      <div>
        <div class="crumbs"><a href="/" id="backLink">← Back to Projects</a></div>
        <h1 id="d-title" style="margin:8px 0 0"></h1>
        <p id="d-desc" class="subtitle" style="margin:6px 0 0"></p>
      </div>
      <div id="d-links"></div>
    </section>
    <section>
      <div class="gallery" id="d-gallery"></div>
    </section>
    <footer>© <span id="year2"></span> Manas Dixit</footer>
  </main>

  <script>
    // Years
    const y = new Date().getFullYear();
    document.getElementById('year').textContent = y;
    document.getElementById('year2').textContent = y;

    // Project data
    const PROJECTS = {
      'dynamic-slam': {
        title:'Dynamic Visual SLAM with SegFormer',
        desc:'SegFormer masks filter moving-object features to stabilize ORB-SLAM3 in dynamic scenes.',
        videos:[
          {id:'SpPrUodWPOo',caption:'Baseline: dynamic ORB-SLAM (no masking)'},
          {id:'uFI8JMBfABI',caption:'With SegFormer: masked keypoints → stable tracking'},
          {id:'rPasoh3TChU',caption:'Static reference run for comparison'}
        ],
        links:[
          {href:'https://github.com/Manasdixit02/Dynamic_visual_SLAM',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1VzBqGeIcddwIwNuP9lUvNKDEmHPApW9bGFcjGzeV1ko/edit?usp=sharing',label:'Slides'}
        ]
      },
      'ctra-ekf': {
        title:'CTRA Motion Model in EKF Sensor Fusion',
        desc:'CTRA improves estimation under high-turn maneuvers vs CA in a ROS 2 sensor-fusion pipeline.',
        videos:[
          {id:'ppGeLjBqUFg',caption:'CTRA demo: tight turn tracking'},
          {id:'vGe8iWsjSqc',caption:'CA demo: comparison run'},
          {id:'2EUyFGxl5i8',caption:'Scenario overview'}
        ],
        links:[]
      },
      'ibvs-tb3': {
        title:'IBVS for TurtleBot3',
        desc:'Image-space control loop with interaction matrix; closed-loop regulation in ROS 2.',
        videos:[
          {id:'02knEVV_Q8w',caption:'Normal conditions: IBVS tracks the target reliably on TurtleBot3'},
          {id:'IyRRwgQm2kk',caption:'Occlusion stress test: standard IBVS loses the target when it’s partially hidden'},
          {id:'0eTwzXML35s',caption:'Occlusion-robust IBVS: feature-estimation integration restores stable tracking'}
        ],
        links:[
          {href:'https://github.com/Manasdixit02/IBVS_for_turtlebot3',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1BU-yxYh0wP_f9XoPwtZVcTX0MRy9yCk05-bUhChLVbc/edit?usp=sharing',label:'Slides'}
        ]
      },

      /* New project entry + LINKS */
      'mono-3d-recon': {
        title:'3D Reconstruction from a Single Image',
        desc:'Monocular depth + projective geometry pipeline: fine-tuned GLPN on KITTI and back-projected depth into a colored 3D point cloud.',
        videos:[],
        links:[
          {href:'https://docs.google.com/presentation/d/1f5-VLVjZl_s8AzUCQtQWq6iG_EVbJrz6/edit?slide=id.p1#slide=id.p1',label:'Slides'},
          {href:'https://github.com/SharvaK/RobotVision_project/tree/main',label:'Repository'}
        ],
      },

      /* Thesis project entry */
      'thesis-cornfield-nav': {
        title:'Thesis Project: Autonomous Navigation in Unstructured Agricultural Environments',
        desc:'Mapping + Nav2 autonomy in Isaac Sim, then expert teleop data collection to train an imitation-learning local planner for unstructured cornfields. Deployment and evaluation are in progress.',
        videos:[],
        links:[]
      },

      'yolov4-adv': {
        title:'YOLOv4 Adversarial Training',
        desc:'Custom PyTorch pipeline; adversarial augmentation and label smoothing for robustness.',
        videos:[],
        links:[
          {href:'https://github.com/NVP12/Adversarial-Defense-Strategies-for-Object-Detection-In-Autonomous-Vehicle-Perception-Systems',label:'Repository'},
          {href:'DL_Final_Report.pdf',label:'Report'}
        ]
      },
      'kinova-rl': {
        title:'RL Control Environment for Kinova Gen3 Lite',
        desc:'Redesigned joint-space RL environment for Kinova Gen3 Lite with curriculum and physics-grounded reward shaping.',
        videos:[],
        links:[
          {href:'https://github.com/Manasdixit02/RL-based-Controller-for-Robotic-Arm-Target-Reaching',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1rQDUz1GlFeZDkG1IdVCdqD9odU3jXVqfQyjvVtsxauM/edit?slide=id.g3ae4ee2b002_0_29#slide=id.g3ae4ee2b002_0_29',label:'Slides'}
        ]
      }
    };

    // Router helpers
    function showHome(){
      document.getElementById('detail').classList.add('hidden');
      document.getElementById('home').classList.remove('hidden');
    }
    function showDetail(id){
      const data = PROJECTS[id];
      if(!data){ showHome(); return; }
      document.getElementById('home').classList.add('hidden');
      document.getElementById('detail').classList.remove('hidden');
      document.getElementById('d-title').textContent = data.title;
      document.getElementById('d-desc').textContent = data.desc || '';
      const linksDiv = document.getElementById('d-links');
      linksDiv.innerHTML = '';
      if(data.links && data.links.length){
        data.links.forEach(l=>{
          const a=document.createElement('a');
          a.className='btn';
          a.href=l.href;
          a.target='_blank';
          a.rel='noreferrer';
          a.textContent=l.label;
          linksDiv.appendChild(a);
        });
      }
      const g = document.getElementById('d-gallery');
      g.innerHTML = '';
      if(data.videos && data.videos.length){
        data.videos.forEach(v=>{
          const card=document.createElement('div'); card.className='vid';
          card.innerHTML = `
            <iframe src="https://www.youtube.com/embed/${v.id}" title="YouTube video" allowfullscreen loading="lazy"></iframe>
            <div class="caption">${v.caption||''}</div>
          `;
          g.appendChild(card);
        });
      }
      else {
        if(id==='yolov4-adv'){
          const card=document.createElement('div');
          card.innerHTML = `<h3>ABSTRACT</h3><p>Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in a wide range of computer vision tasks, including image classification, object detection, and segmentation, owing to advances in training techniques and architectural innovations. However, their susceptibility to adversarial attacks raises serious concerns about their reliability, particularly in safety-critical applications such as autonomous driving. While adversarial training has emerged as a promising defense, its effectiveness is typically limited to the types of attacks seen during training. Moreover, conventional training pipelines terminate learning after convergence, lacking the adaptability to counter evolving adversarial threats encountered during real-world deployment. This project addresses these limitations in the context of object detection for autonomous vehicle perception systems, focusing on CNN-based detectors such as YOLOv4. We propose a self-supervised continual learning pipeline that enables the detector to adapt and improve during deployment by leveraging adversarial inputs in real time. Our input dataset consists of both clean and adversarial examples generated using Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). We incorporate the MagNet detector to identify adversarial samples during inference, which are then used for training the target YOLOv4 via self-supervised learning mechanism. This mechanism allows the YOLOv4 model to incrementally enhance its robustness through continuous self-supervised updates. Through this adaptive training pipeline, we demonstrate improved resilience of YOLOv4 to adversarial perturbations, achieving a measurable increase in mean Average Precision (mAP) under FGSM and PGD attacks.</p>`;
          g.appendChild(card);

        } else if(id==='mono-3d-recon'){
          const card=document.createElement('div');
          card.className='vid';
          card.innerHTML = `
            <div class="caption" style="border-top:none">
              <h3 style="margin:0 0 10px">3D Reconstruction from a Single Image</h3>

              <!-- Image + Video side-by-side -->
              <div class="media-row">
                <div class="media">
                  <img
                    src="https://raw.githubusercontent.com/SharvaK/RobotVision_project/main/bird1.jpg"
                    alt="3D Reconstruction Result"
                    loading="lazy"
                  />
                </div>
                <div class="media">
                  <iframe
                    src="https://www.youtube.com/embed/iy5rMMb7dKg"
                    title="3D Reconstruction Video"
                    allowfullscreen
                    loading="lazy"
                  ></iframe>
                </div>
              </div>

              <h4>Overview</h4>
              <p>This project investigates monocular 3D reconstruction, where a dense 3D representation of a scene or object is inferred from a single RGB image. The goal is to enable depth and geometry perception in scenarios where stereo cameras or depth sensors are unavailable or impractical, such as lightweight robots, drones, and augmented-reality systems. The pipeline leverages modern deep learning–based depth estimation and classical projective geometry to bridge 2D visual input and 3D spatial understanding.</p>

              <h4>Problem Statement</h4>
              <p>Given a single RGB image captured by a monocular camera, estimate: (1) a dense depth map of the scene, and (2) a corresponding 3D point cloud using known camera geometry. This is an ill-posed problem due to scale ambiguity and missing geometric constraints.</p>

              <h4>Methodology</h4>
              <h5 style="margin:10px 0 6px">1. Monocular Depth Estimation</h5>
              <p>Fine-tuned a pretrained transformer-based depth estimation model (GLPN) on the KITTI depth dataset. Input: single RGB image resized to 256×256. Output: dense per-pixel depth prediction. Supervision: ground-truth depth maps from KITTI, scaled appropriately from 16-bit depth encoding. Loss function: Mean Squared Error (MSE) between predicted and ground-truth depth.</p>

              <h5 style="margin:10px 0 6px">2. 2D-to-3D Back-Projection</h5>
              <p>Modeled the camera using a pinhole camera model. Each pixel (u, v) with predicted depth Z was back-projected into 3D space using camera intrinsics, and the original RGB value was associated with each 3D point to generate a colored point cloud.</p>

              <h5 style="margin:10px 0 6px">3. Post-processing</h5>
              <p>Removed redundant or invalid depth points and visualized both depth predictions and the reconstructed 3D geometry for qualitative evaluation.</p>

              <h4>Dataset</h4>
              <p>KITTI Depth Dataset: real-world outdoor driving scenes with sparse LiDAR-based ground-truth depth projected into the camera frame.</p>

              <h4>Results</h4>
              <p>Successfully reconstructed scene-level 3D point clouds from a single RGB image. Depth predictions captured large-scale scene geometry (roads, vehicles, buildings) and demonstrated feasibility of monocular 3D reconstruction without explicit stereo or LiDAR input.</p>

              <h4>Challenges</h4>
              <ul>
                <li>Depth ambiguity and scale sensitivity inherent to monocular vision.</li>
                <li>Accurate alignment between predicted depth resolution and ground-truth depth maps.</li>
                <li>High computational cost of fine-tuning transformer-based vision models.</li>
                <li>No explicit surface connectivity — output limited to point clouds rather than full meshes.</li>
              </ul>

              <h4>Future Work</h4>
              <ul>
                <li>Shape completion and surface reconstruction to generate watertight 3D meshes from partial point clouds.</li>
                <li>Incorporate multi-view consistency when sequential images are available.</li>
                <li>Improve geometric consistency using smoothness and edge-aware depth losses.</li>
              </ul>

              <h4>Key Takeaways</h4>
              <ul>
                <li>Built an end-to-end monocular 3D reconstruction pipeline combining deep learning and projective geometry.</li>
                <li>Hands-on experience with transformer-based depth models, depth supervision, and 3D back-projection.</li>
                <li>Strengthened understanding of the gap between 2D perception and 3D spatial reasoning in real-world settings.</li>
              </ul>
            </div>
          `;
          g.appendChild(card);

        } else if(id==='kinova-rl') {
          const card=document.createElement('div');
          card.className='vid';
          card.innerHTML = `
            <iframe src="https://www.youtube.com/embed/vhEqvj8y0g4" title="YouTube video" allowfullscreen loading="lazy"></iframe>
            <div class="caption">Kinova Gen3 Lite: RL policy (trained in my modified environment) performing target-reaching control.</div>
            <div class="caption" style="border-top:none">
              <p style="margin:0 0 8px;font-size:18px;color:var(--muted)">
                Checkout the slides for full results and more graphics.
              </p>
              <h3 style="margin-top:0">Project Summary</h3>

              <h4>Objective</h4>
              <p>Build a fully redesigned RL environment for the Kinova Gen3 Lite robotic arm in PyBullet, enabling true continuous-control learning instead of IK-driven motion.</p>

              <h4>Reference Baseline</h4>
              <p>The redesign is based on a critical analysis of the original UR5 environment: <a href="https://github.com/leesweqq/ur5_reinforcement_learning_grasp_object/blob/main/ur5_env.py" target="_blank" rel="noreferrer">UR5 RL Environment</a>.</p>

              <h4>Problems With the Original UR5 Environment</h4>
              <ul>
                <li>Used inverse kinematics (IK) inside the control loop, so the agent did not actually learn robot motion — IK solved most of the task.</li>
                <li>Provided non-Markovian observations (e.g., missing joint states, velocities, and end-effector pose).</li>
                <li>Defined an action space that bypassed robot dynamics by directly commanding end-effector positions.</li>
                <li>Rewards primarily reflected IK convergence rather than meaningful control behavior learned by the policy.</li>
                <li>As a result, it was not suitable for genuine RL-based control learning.</li>
              </ul>

              <h4>Key Improvements in the Redesigned Environment</h4>
              <h5>Joint-Space Continuous Control</h5>
              <ul>
                <li>The agent outputs 6D joint-angle increments (Δq) at each time step.</li>
                <li>These increments are applied directly to the robot under joint limits.</li>
                <li>There is no IK or motion planner inside the RL loop — only low-level position control.</li>
              </ul>

              <h5>18-Dimensional Markov Observation Space</h5>
              <ul>
                <li>Joint angles: 6</li>
                <li>Joint velocities: 6</li>
                <li>End-effector position: 3</li>
                <li>Relative goal vector (goal − end-effector): 3</li>
                <li>This provides full access to both robot dynamics and task geometry, making the environment Markovian.</li>
              </ul>

              <h4>Physics-Grounded Reward Function</h4>
              <h5>Distance shaping</h5>
              <ul>
                <li>Separate penalties for far and near distances.</li>
                <li>A deadband near the goal where distance penalties vanish.</li>
              </ul>

              <h5>Safety penalties</h5>
              <ul>
                <li>Collision penalties for self-collision or unsafe proximity.</li>
                <li>Joint-limit proximity penalties to discourage configurations near limits.</li>
              </ul>

              <h5>Smoothness penalties</h5>
              <ul>
                <li>Penalize action magnitude ‖Δq‖².</li>
                <li>Penalize action change ‖Δqₜ − Δqₜ₋₁‖² for jerk reduction.</li>
              </ul>

              <h5>Success incentives</h5>
              <ul>
                <li>A success bonus when the end-effector enters a small threshold around the goal.</li>
                <li>A time-based bonus encouraging faster convergence.</li>
              </ul>

              <p>Together, these components promote accurate, stable, smoothness, and safe multi-joint motion.</p>

              <h4>Curriculum Learning Framework</h4>
              <ul>
                <li>Uses a five-stage curriculum to gradually increase difficulty:
                  <ul>
                    <li>Easy, near-center goals.</li>
                    <li>Stages 2–4: expanding coverage over different regions of the workspace.</li>
                    <li>Fully random goals across the reachable workspace.</li>
                  </ul>
                </li>
                <li>Curriculum progression is controlled via episode thresholds such as [50, 100, 150, 200].</li>
                <li>This improves training stability, sample efficiency, and final performance.</li>
              </ul>

              <h4>Outcome</h4>
              <ul>
                <li>The redesigned Kinova Gen3 Lite environment enables true continuous-control RL, where the agent:
                  <ul>
                    <li>Learns coordinated multi-joint motion under realistic dynamics and constraints.</li>
                    <li>No longer relies on IK shortcuts to solve the task.</li>
                  </ul>
                </li>
                <li>It serves as a research-grade benchmark for:
                  <ul>
                    <li>Robotic reaching,</li>
                    <li>Future grasping and manipulation tasks,</li>
                    <li>Multi-modal (e.g., vision + proprioception) policy learning,
                    </li>
                    <li>And sim-to-real transfer studies.</li>
                  </ul>
                </li>
              </ul>
            </div>
          `;
          g.appendChild(card);
        } else if(id==='thesis-cornfield-nav'){
          const card=document.createElement('div');
          card.className='vid';
          card.innerHTML = `
            <div class="caption" style="border-top:none">
              <h3 style="margin:0 0 10px">Thesis Project: Autonomous Navigation in Unstructured Agricultural Environments</h3>

              <h4>Stage 1: Mapping</h4>
              <div class="media-row">
                <div class="media">
                  <iframe
                    src="https://www.youtube.com/embed/JlAoLlOukuA"
                    title="Stage 1: Mapping Demo"
                    allowfullscreen
                    loading="lazy"
                  ></iframe>
                </div>
              </div>
              <p>The first stage of this thesis focuses on creating a reliable mapping pipeline in a simulated agricultural environment.</p>
              <p>A virtual cornfield was built in NVIDIA Isaac Sim to replicate the structure and constraints of real crop rows. A ground rover was spawned in this environment and equipped with a LiDAR sensor and an RGB camera for perception. (A demo video shows the rover operating inside the simulated cornfield.)</p>
              <p>To enable controlled exploration, a custom ROS 2 teleoperation node was developed. This node reads keyboard inputs from the user and sends velocity commands to the rover in Isaac Sim, allowing manual navigation through the field. During teleoperation, the simulator publishes RGB images, LiDAR scans, and odometry data over ROS 2 topics.</p>
              <p>For mapping, SLAM Toolbox was launched in online asynchronous mode, allowing the map to be built in real time as the rover explored the environment. After sufficient coverage, the map_saver_cli node from the Nav2 map_server package was used to save the generated occupancy grid map for later use.</p>
              <p>This stage establishes a complete simulation-to-ROS 2 mapping workflow tailored for agricultural environments.</p>

              <h4>Stage 2: Autonomous Path Planning and Navigation using ROS 2 Nav2</h4>
              <div class="media-row">
                <div class="media">
                  <iframe
                    src="https://www.youtube.com/embed/KbQgsVpca_4"
                    title="Stage 2: Nav2 Autonomous Navigation Demo"
                    allowfullscreen
                    loading="lazy"
                  ></iframe>
                </div>
              </div>
              <p>In the second stage, the saved map from Stage 1 was used to enable fully autonomous navigation using the ROS 2 Navigation (Nav2) stack.</p>
              <p>The Isaac Sim cornfield environment was launched with the rover and its sensors active, followed by the Nav2 bringup pipeline, which loads the occupancy grid map and initializes localization, global planning, local planning, and control modules.</p>
              <p>Autonomous navigation was executed through RViz, where the rover was first localized by setting an initial pose on the map. A goal pose was then specified, after which Nav2 automatically generated a global path and commanded the rover to navigate toward the target. (A screen-recorded video demonstrates the rover autonomously planning and moving through the environment.)</p>
              <p>While effective in structured settings, experiments showed that classical costmap-based local planners struggle in highly unstructured agricultural environments, motivating a shift toward learning-based local planning.</p>

              <h4>Stage 3: Expert Data Collection for Imitation Learning</h4>
              <div class="media-row">
                <div class="media">
                  <img
                    src="loss_curve_multi.png"
                    alt="Imitation learning loss curves (train/val/test)"
                    loading="lazy"
                  />
                </div>
              </div>
              <p>To address these limitations, the third stage focuses on collecting expert demonstrations for imitation learning–based local planning.</p>
              <p>A ROS 2 teleoperation node was used to manually operate the rover inside the simulated cornfield, deliberately generating diverse expert behaviors such as straight-row traversal, curved path following, obstacle avoidance, speed modulation based on free space, and U-turn maneuvers for row transitions.</p>
              <p>During teleoperation, multi-modal data was recorded using rosbag, including control commands (geometry_msgs/Twist), RGB images, LiDAR scans, odometry, and goal pose information. These rosbags were then post-processed offline to align sensor observations with expert actions, forming structured datasets for supervised learning. An imitation learning policy was trained on this data, with performance monitored using training, validation, and test loss curves.</p>
              <p>This stage enables the transition from rule-based navigation to a data-driven local planner capable of capturing human driving intuition in unstructured environments.</p>

              <h4>Stage 4: Learned Policy Deployment and Evaluation (In Progress)</h4>
              <p>The fourth stage focuses on deploying the trained imitation learning policy within the navigation stack and evaluating its performance in simulation. This includes integrating the learned policy as a local planner, benchmarking it against classical planners, and evaluating metrics such as path tracking quality, collision avoidance, smoothness, and robustness across diverse cornfield layouts.</p>
              <p>This stage is currently in progress and will be updated as experiments are completed.</p>

              <h4>Next Step: Real-World Deployment</h4>
              <p>Following successful simulation-based evaluation, the final phase of this work will involve deploying the learned policy on the physical ground rover and evaluating its performance in real agricultural environments, closing the simulation-to-real (sim-to-real) loop.</p>
            </div>
          `;
          g.appendChild(card);
        } else {
          const p=document.createElement('p');
          p.className='subtitle';
          p.textContent='No videos yet. Check the repository above for details.';
          g.appendChild(p);
        }
      }
      document.getElementById('backLink').href = location.pathname + '#projects';
    }

    // Wire up details buttons
    document.querySelectorAll('.details').forEach(btn=>{
      btn.addEventListener('click', ()=>{
        const id = btn.getAttribute('data-project');
        history.pushState({project:id}, '', `?project=${id}`);
        showDetail(id);
      });
    });

    // Initial route
    const qs = new URLSearchParams(location.search);
    const projectId = qs.get('project');
    if(projectId){ showDetail(projectId); } else { showHome(); }

    // popstate
    window.addEventListener('popstate', ()=>{
      const id=(new URLSearchParams(location.search)).get('project');
      if(id) showDetail(id); else showHome();
    });

    // Skills: tools/software only (from resume)
    const tools = [
      {name:'ROS 2', slug:'ros'},
      {name:'ROS 1', slug:'ros'},
      {name:'RViz', slug:''},
      {name:'Gazebo', slug:'gazebo'},
      {name:'Docker', slug:'docker'},
      {name:'Git', slug:'git'},
      {name:'Jira', slug:'jira'},
      {name:'WSL2', slug:'windows'},
      {name:'MATLAB', slug:'mathworks'},
      {name:'Simulink', slug:'mathworks'},
      {name:'OpenCV', slug:'opencv'},
      {name:'PyTorch', slug:'pytorch'},
      {name:'TensorFlow', slug:'tensorflow'},
      {name:'LabVIEW', slug:'labview'},
      {name:'CARLA', slug:''},
      {name:'CUDA', slug:'nvidia'},
      {name:'CANoe', slug:''},
      {name:'CANalyzer', slug:''},
      {name:'Vector', slug:''}
    ];

    const skillsGrid = document.getElementById('skillsGrid');
    const icon = (slug)=> slug ? `https://cdn.simpleicons.org/${slug}` : '';

    tools.forEach(t=>{
      const el = document.createElement('div'); el.className='tool';
      const iSrc = icon(t.slug);
      if(iSrc){
        const img = document.createElement('img'); img.src = iSrc; img.alt = `${t.name} logo`;
        img.onerror = ()=>{ img.remove(); el.classList.add('pill'); el.textContent = t.name; };
        el.appendChild(img);
        const span = document.createElement('span'); span.textContent = t.name; el.appendChild(span);
      } else {
        el.className = 'pill'; el.textContent = t.name;
      }
      skillsGrid.appendChild(el);
    });
  </script>
</body>
</html>
