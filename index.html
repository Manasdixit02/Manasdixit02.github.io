<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Manas | Robotics Perception, Cognition & Controls Algorithms</title>
  <meta name="description" content="Portfolio of Manas Dixit — robotics, state estimation, SLAM, Controls and perception projects." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    :root{ --bg:#f5f7fb; --card:#ffffff; --ink:#0f172a; --muted:#475569; --line:#e2e8f0; --brand:#2563eb; --brand2:#14b8a6; --radius:14px; --shadow:0 6px 18px rgba(2,6,23,.08); }
    *{box-sizing:border-box}
    html,body{height:100%}
    html{scroll-behavior:smooth}
    body{margin:0;font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;background:var(--bg);color:var(--ink);line-height:1.6}
    a{color:var(--brand);text-decoration:none}
    a:hover{text-decoration:underline}
    .container{max-width:1080px;margin:0 auto;padding:0 20px}

    /* NAV */
    .nav{position:sticky;top:0;background:rgba(255,255,255,.9);backdrop-filter:blur(8px);border-bottom:1px solid var(--line);z-index:50}
    .nav-inner{display:flex;align-items:center;justify-content:space-between;padding:14px 0}
    .brand{font-weight:800;letter-spacing:.2px}
    .nav-links{display:flex;gap:12px;flex-wrap:wrap}
    .btn{display:inline-flex;align-items:center;gap:8px;border:1px solid var(--line);background:#fff;color:var(--ink);padding:8px 12px;border-radius:999px;box-shadow:var(--shadow);cursor:pointer}
    .btn:hover{background:#eef2ff}

    /* HERO */
    .hero{padding:56px 0}
    .hero h1{font-size:clamp(34px,5vw,60px);line-height:1.1;margin:0 0 10px}
    .hero p{color:var(--muted);margin:8px 0 18px}
    .chips{margin-top:20px}
    .chip-group{margin-bottom:20px}
    .chip-group h3{margin:0 0 8px;font-size:16px;font-weight:600;color:var(--ink)}
    .chip-list{display:flex;flex-wrap:wrap;gap:8px}
    .chip{font-size:12px;color:#0f172a;background:#e0e7ff;border:1px solid #c7d2fe;padding:6px 9px;border-radius:999px}

    /* SECTIONS */
    section{padding:36px 0}
    .title{font-size:26px;margin:0 0 10px}
    .subtitle{color:var(--muted);margin:-2px 0 18px}

    /* PROJECTS GRID */
    .grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:18px}
    .card{background:var(--card);border:1px solid var(--line);border-radius:var(--radius);overflow:hidden;box-shadow:var(--shadow)}
    .thumb{width:100%;aspect-ratio:16/9;object-fit:cover;border-bottom:1px solid var(--line)}
    .card-body{padding:14px 14px 18px}
    .tags{margin-top:6px}
    .tag{font-size:12px;color:#334155;background:#f1f5f9;border:1px solid #e2e8f0;padding:4px 8px;border-radius:999px;margin-right:6px}
    .row-end{display:flex;justify-content:flex-end;margin-top:12px}

    /* FIX 1: Make ONLY selected cards narrower */
    .card.narrow{max-width:380px}

    /* FIX 2: Make thumbnail naturally shorter (scales with width, no weird cropping) */
    .thumb.sm{aspect-ratio:16/8}

    /* FIX 3: Description font size matches subheading size */
    .card-body p{margin:8px 0 18px;font-size:15px;line-height:1.5;color:var(--muted)}
    .card-body h3{font-size:18px;line-height:1.25;font-weight:600;margin:0 0 6px;}

    /* PROJECT CATEGORY HEADERS */
    .proj-block{margin-top:18px}
    .proj-h{display:flex;align-items:baseline;justify-content:space-between;gap:12px;margin:22px 0 10px}
    .proj-h h3{margin:0;font-size:18px}
    .proj-h p{margin:0;color:var(--muted);font-size:14px}
    .proj-sub{margin:14px 0 10px}
    .proj-sub h4{margin:0 0 10px;font-size:15px;color:var(--ink);font-weight:650}
    .divider{height:1px;background:var(--line);margin:18px 0}

    /* DETAIL PAGE */
    main.hidden{display:none}
    .detail-head{display:flex;flex-wrap:wrap;align-items:flex-end;justify-content:space-between;gap:16px;margin:6px 0 18px}
    .crumbs{color:var(--muted)}
    .gallery{display:grid;grid-template-columns:repeat(auto-fit,minmax(320px,1fr));gap:16px}
    .vid{background:var(--card);border:1px solid var(--line);border-radius:var(--radius);overflow:hidden}
    .vid iframe{width:100%;aspect-ratio:16/9;border:0;display:block}
    .caption{padding:10px 12px;font-size:14px;color:var(--muted);border-top:1px solid var(--line)}

    /* ABOUT & CONTACT */
    .about{display:grid;grid-template-columns:1.2fr .8fr;gap:18px}
    @media (max-width:900px){.about{grid-template-columns:1fr}}
    footer{border-top:1px solid var(--line);color:var(--muted);padding:16px 0;margin-top:32px}

    /* SKILLS */
    .skills-logos{display:grid;grid-template-columns:repeat(auto-fit,minmax(140px,1fr));gap:14px}
    .tool{display:flex;align-items:center;gap:10px;background:var(--card);border:1px solid var(--line);border-radius:12px;padding:10px}
    .tool img{width:22px;height:22px}
    .tool span{font-size:14px}
    .pill{display:inline-flex;align-items:center;border:1px dashed var(--line);padding:8px 10px;border-radius:999px;color:var(--muted)}
  </style>
</head>
<body>
  <nav class="nav">
    <div class="container nav-inner">
      <div class="brand">Manas | Robotics Perception, Cognition & Controls Algorithms</div>
      <div class="nav-links">
        <a class="btn" href="#about">About</a>
        <a class="btn" href="#projects">Projects</a>
        <a class="btn" href="#skills">Skills</a>
        <a class="btn" href="#publications">Publications</a>
        <a class="btn" href="#contact">Contact</a>
        <a class="btn" href="Manas_dixit_resume_full_time_20250915.pdf" download="Manas_Dixit_Resume.pdf">Resume</a>
      </div>
    </div>
  </nav>

  <main id="home" class="container">
    <section class="hero">
      <h1>Hi, I'm Manas — building reliable robot perception, cognition and control algorithms.</h1>
      <p>MS Robotics @ UMN · Algorithms Development Intern @ Magna (ADAS) · ex-SEDEMAC Controls. Building end-to-end robotics software: SLAM in dynamic scenes, EKF/UKF pipelines, RL-tuned control, and ROS 2/C++/Python tooling for embedded and autonomous systems.</p>

      <div class="chips">
        <div class="chip-group">
          <h3>Estimation & Controls</h3>
          <div class="chip-list">
            <span class="chip">EKF / UKF</span>
            <span class="chip">Bayesian Filtering</span>
            <span class="chip">PID / Optimal Control</span>
            <span class="chip">Sensor Fusion</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Perception & AI</h3>
          <div class="chip-list">
            <span class="chip">Visual SLAM</span>
            <span class="chip">YOLOv8</span>
            <span class="chip">SegFormer</span>
            <span class="chip">3D Scene Reconstruction</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Robotics Software</h3>
          <div class="chip-list">
            <span class="chip">ROS 2</span>
            <span class="chip">Gazebo • RViz</span>
            <span class="chip">Inverse Kinematics</span>
            <span class="chip">Teleoperation</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Programming & Tools</h3>
          <div class="chip-list">
            <span class="chip">C++ • Python</span>
            <span class="chip">PyTorch • TensorFlow</span>
            <span class="chip">Docker • WSL2</span>
            <span class="chip">Git • JIRA</span>
          </div>
        </div>

        <div class="chip-group">
          <h3>Hardware & Sensors</h3>
          <div class="chip-list">
            <span class="chip">Radar • Camera • LiDAR</span>
            <span class="chip">IMU • Wheel Encoders</span>
            <span class="chip">Kinova Gen3</span>
            <span class="chip">TurtleBot3</span>
          </div>
        </div>
      </div>

    </section>

    <!-- ABOUT before projects -->
    <section id="about">
      <h2 class="title">About</h2>
      <div class="about">
        <div>
          <p>I’m a Robotics MS student working across estimation, perception, and control. Projects span CTRA/CA-based EKF pipelines (Magna ADAS), dynamic visual SLAM with semantic filtering, and ROS 2 navigation + IK for an agricultural rover with a 3-DOF arm.</p>
          <p>I enjoy building robotics systems end-to-end, transforming concepts and simulations into reliable pipelines validated in real-world environments.</p>
        </div>
        <div>
          <div class="chips"><span class="chip">Open to 2026 roles</span><span class="chip">US-based</span></div>
        </div>
      </div>
    </section>

    <!-- PROJECTS (structured) -->
    <section id="projects">
      <h2 class="title">Projects</h2>
      <p class="subtitle">Grouped by domain to make scanning easier.</p>

      <!-- 1) AUTONOMY -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Autonomy</h3>
          <p>SLAM • Sensor Fusion • Navigation</p>
        </div>

        <div class="proj-sub">
          <h4>SLAM</h4>
          <div class="grid">
            <article class="card narrow">
              <img class="thumb sm" src="https://img.youtube.com/vi/uFI8JMBfABI/0.jpg" alt="Dynamic SLAM"/>
              <div class="card-body">
                <h3>Dynamic Visual SLAM with SegFormer</h3>
                <p>ORB-SLAM3 + SegFormer masks for robust tracking in dynamic environments.</p>
                <div class="tags"><span class="tag">VIO/SLAM</span><span class="tag">SegFormer</span><span class="tag">C++</span></div>
                <div class="row-end"><button class="btn details" data-project="dynamic-slam">→ details</button></div>
              </div>
            </article>
          </div>
        </div>

        <div class="proj-sub">
          <h4>Sensor Fusion</h4>
          <div class="grid">
            <article class="card narrow">
              <img class="thumb sm" src="https://img.youtube.com/vi/2EUyFGxl5i8/0.jpg" alt="CTRA vs CA"/>
              <div class="card-body">
                <h3>CTRA Motion Model in EKF Sensor Fusion</h3>
                <p>Sharper maneuver handling than CA; tuned Q/R and Jacobians in a ROS 2 fusion pipeline.</p>
                <div class="tags"><span class="tag">EKF</span><span class="tag">CTRA</span><span class="tag">ROS 2</span></div>
                <div class="row-end"><button class="btn details" data-project="ctra-ekf">→ details</button></div>
              </div>
            </article>
          </div>
        </div>

        <div class="proj-sub">
          <h4>Navigation</h4>
          <p>Coming soon</p>
          <div class="grid">
            <!-- intentionally blank -->
          </div>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 2) PERCEPTION -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Perception</h3>
          <p>Vision-based control and sensing pipelines</p>
        </div>

        <div class="grid">
          <article class="card narrow">
            <img class="thumb sm" src="https://img.youtube.com/vi/02knEVV_Q8w/0.jpg" alt="IBVS"/>
            <div class="card-body">
              <h3>IBVS for TurtleBot3</h3>
              <p>Image-space control loop with interaction matrix and closed-loop regulation.</p>
              <div class="tags"><span class="tag">Control</span><span class="tag">ROS 2</span></div>
              <div class="row-end"><button class="btn details" data-project="ibvs-tb3">→ details</button></div>
            </div>
          </article>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 3) AI -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>AI</h3>
          <p>Deep learning • Reinforcement learning</p>
        </div>

        <div class="grid">
          <article class="card">
            <img class="thumb" src="https://dummyimage.com/800x450/0d152b/cfe0ff&text=YOLOv4+Training" alt="YOLOv4"/>
            <div class="card-body">
              <h3>YOLOv4 Adversarial Training</h3>
              <p>Custom PyTorch pipeline for robust detection via adversarial augmentation.</p>
              <div class="tags"><span class="tag">PyTorch</span><span class="tag">Detection</span></div>
              <div class="row-end"><button class="btn details" data-project="yolov4-adv">→ details</button></div>
            </div>
          </article>

          <article class="card">
            <img class="thumb" src="https://img.youtube.com/vi/vhEqvj8y0g4/0.jpg" alt="Kinova Gen3 RL Control"/>
            <div class="card-body">
              <h3>RL Control Environment for Kinova Gen3 Lite</h3>
              <p>Joint-space continuous control environment with curriculum and physics-grounded reward shaping.</p>
              <div class="tags"><span class="tag">Reinforcement Learning</span><span class="tag">Kinova Gen3</span><span class="tag">PyBullet</span></div>
              <div class="row-end"><button class="btn details" data-project="kinova-rl">→ details</button></div>
            </div>
          </article>
        </div>
      </div>

      <div class="divider"></div>

      <!-- 4) CONTROLS -->
      <div class="proj-block">
        <div class="proj-h">
          <h3>Controls</h3>
          <p>Coming soon</p>
        </div>

        <div class="grid">
          <!-- intentionally blank -->
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS (NEW SECTION) -->
    <section id="publications">
      <h2 class="title">Publications</h2>
      <div class="card" style="padding:16px">
        <h3 style="margin:0 0 8px;font-size:18px;line-height:1.25;font-weight:600;">Modelling of a Transient VCRS and Virtual Optimal Charge Determination for Automobiles</h3>
        <p style="margin:0 0 12px;font-size:15px;line-height:1.5;color:var(--muted)">
          Experimental research on the Vapor Compression Refrigeration System (VCRS) used in automobiles and other air conditioning systems is abundant in literature but it comes with inherent problems like the cost of the setup, time consumption, and tediousness of the process. Therefore, the objective of this study is to develop the fully-fledged transient or dynamic model of R134a based VCRS and further utilize it to virtually determine the optimal refrigerant charge quantity. The Simulink model developed in this work is an integration of physics-based mathematical models for each component (compressor, heat exchangers, and expansion valve), working together in a cycle where the compressor dictates the transient process. A sample simulation has been performed by taking 5 bar as the initial pressure of suction and discharge tanks and the steady state results (Discharge pressure, suction pressure, refrigerant mass flow rate, superheat and subcool temperatures) obtained. The results are found to closely follow the similar trends as the experimental results, hence validating the correctness of the Simulink model. Further, Using the proposed transient model, simulations have been performed for different initial state conditions and steady state plots (discharge pressure V/s charge, cooling capacity V/s charge, COP V/s charge, Subcool and Superheat V/s charge) have been constructed. Analysis of these plots shows that the values of cooling capacity and COP attain maxima simultaneously corresponding to 650 grams of charge. Hence, using maximum COP method for optimal charge determination, the optimal charge quantity has been calculated to be around 650 grams. In the end, this work hopes to provide researchers a transient model that can be used as a cost-effective substitute of the experimental process.
        </p>
        <div class="row-end">
          <a class="btn" target="_blank" rel="noreferrer" href="https://www.sae.org/papers/modelling-a-transient-vcrs-virtual-optimal-charge-determination-automobiles-2021-28-0255">Read paper</a>
        </div>
      </div>
    </section>

    <!-- SKILLS (tools/software only, with logos) -->
    <section id="skills">
      <h2 class="title">Skills — Tools & Software</h2>
      <div class="skills-logos" id="skillsGrid"></div>
    </section>

    <!-- CONTACT -->
    <section id="contact">
      <h2 class="title">Contact</h2>
      <div class="card" style="padding:16px">
        <p>Email: <a href="mailto:manasdixit13@gmail.com">manasdixit13@gmail.com</a> · LinkedIn: <a target="_blank" rel="noreferrer" href="https://www.linkedin.com/in/manas2/">/in/manas2</a> · GitHub: <a target="_blank" rel="noreferrer" href="https://github.com/Manasdixit02">@Manasdixit02</a></p>
      </div>
    </section>

    <footer>© <span id="year"></span> Manas Dixit</footer>
  </main>

  <!-- DETAIL (Project page) -->
  <main id="detail" class="container hidden">
    <section class="detail-head">
      <div>
        <div class="crumbs"><a href="/" id="backLink">← Back to Projects</a></div>
        <h1 id="d-title" style="margin:8px 0 0"></h1>
        <p id="d-desc" class="subtitle" style="margin:6px 0 0"></p>
      </div>
      <div id="d-links"></div>
    </section>
    <section>
      <div class="gallery" id="d-gallery"></div>
    </section>
    <footer>© <span id="year2"></span> Manas Dixit</footer>
  </main>

  <script>
    // Years
    const y = new Date().getFullYear();
    document.getElementById('year').textContent = y;
    document.getElementById('year2').textContent = y;

    // Project data
    const PROJECTS = {
      'dynamic-slam': {
        title:'Dynamic Visual SLAM with SegFormer',
        desc:'SegFormer masks filter moving-object features to stabilize ORB-SLAM3 in dynamic scenes.',
        videos:[
          {id:'SpPrUodWPOo',caption:'Baseline: dynamic ORB-SLAM (no masking)'},
          {id:'uFI8JMBfABI',caption:'With SegFormer: masked keypoints → stable tracking'},
          {id:'rPasoh3TChU',caption:'Static reference run for comparison'}
        ],
        links:[
          {href:'https://github.com/Manasdixit02/Dynamic_visual_SLAM',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1VzBqGeIcddwIwNuP9lUvNKDEmHPApW9bGFcjGzeV1ko/edit?usp=sharing',label:'Slides'}
        ]
      },
      'ctra-ekf': {
        title:'CTRA Motion Model in EKF Sensor Fusion',
        desc:'CTRA improves estimation under high-turn maneuvers vs CA in a ROS 2 sensor-fusion pipeline.',
        videos:[
          {id:'ppGeLjBqUFg',caption:'CTRA demo: tight turn tracking'},
          {id:'vGe8iWsjSqc',caption:'CA demo: comparison run'},
          {id:'2EUyFGxl5i8',caption:'Scenario overview'}
        ],
        links:[]
      },
      'ibvs-tb3': {
        title:'IBVS for TurtleBot3',
        desc:'Image-space control loop with interaction matrix; closed-loop regulation in ROS 2.',
        videos:[
          {id:'02knEVV_Q8w',caption:'Normal conditions: IBVS tracks the target reliably on TurtleBot3'},
          {id:'IyRRwgQm2kk',caption:'Occlusion stress test: standard IBVS loses the target when it’s partially hidden'},
          {id:'0eTwzXML35s',caption:'Occlusion-robust IBVS: feature-estimation integration restores stable tracking'}
        ],
        links:[
          {href:'https://github.com/Manasdixit02/IBVS_for_turtlebot3',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1BU-yxYh0wP_f9XoPwtZVcTX0MRy9yCk05-bUhChLVbc/edit?usp=sharing',label:'Slides'}
        ]
      },
      'yolov4-adv': {
        title:'YOLOv4 Adversarial Training',
        desc:'Custom PyTorch pipeline; adversarial augmentation and label smoothing for robustness.',
        videos:[],
        links:[
          {href:'https://github.com/NVP12/Adversarial-Defense-Strategies-for-Object-Detection-In-Autonomous-Vehicle-Perception-Systems',label:'Repository'},
          {href:'DL_Final_Report.pdf',label:'Report'}
        ]
      },
      'kinova-rl': {
        title:'RL Control Environment for Kinova Gen3 Lite',
        desc:'Redesigned joint-space RL environment for Kinova Gen3 Lite with curriculum and physics-grounded reward shaping.',
        videos:[],
        links:[
          {href:'https://github.com/Manasdixit02/RL-based-Controller-for-Robotic-Arm-Target-Reaching',label:'Repository'},
          {href:'https://docs.google.com/presentation/d/1rQDUz1GlFeZDkG1IdVCdqD9odU3jXVqfQyjvVtsxauM/edit?slide=id.g3ae4ee2b002_0_29#slide=id.g3ae4ee2b002_0_29',label:'Slides'}
        ]
      }
    };

    // Router helpers
    function showHome(){
      document.getElementById('detail').classList.add('hidden');
      document.getElementById('home').classList.remove('hidden');
    }
    function showDetail(id){
      const data = PROJECTS[id];
      if(!data){ showHome(); return; }
      document.getElementById('home').classList.add('hidden');
      document.getElementById('detail').classList.remove('hidden');
      document.getElementById('d-title').textContent = data.title;
      document.getElementById('d-desc').textContent = data.desc || '';
      const linksDiv = document.getElementById('d-links');
      linksDiv.innerHTML = '';
      if(data.links && data.links.length){
        data.links.forEach(l=>{
          const a=document.createElement('a');
          a.className='btn';
          a.href=l.href;
          a.target='_blank';
          a.rel='noreferrer';
          a.textContent=l.label;
          linksDiv.appendChild(a);
        });
      }
      const g = document.getElementById('d-gallery');
      g.innerHTML = '';
      if(data.videos && data.videos.length){
        data.videos.forEach(v=>{
          const card=document.createElement('div'); card.className='vid';
          card.innerHTML = `
            <iframe src="https://www.youtube.com/embed/${v.id}" title="YouTube video" allowfullscreen loading="lazy"></iframe>
            <div class="caption">${v.caption||''}</div>
          `;
          g.appendChild(card);
        });
      }
      else {
        if(id==='yolov4-adv'){
          const card=document.createElement('div');
          card.innerHTML = `<h3>ABSTRACT</h3><p>Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in a wide range of computer vision tasks, including image classification, object detection, and segmentation, owing to advances in training techniques and architectural innovations. However, their susceptibility to adversarial attacks raises serious concerns about their reliability, particularly in safety-critical applications such as autonomous driving. While adversarial training has emerged as a promising defense, its effectiveness is typically limited to the types of attacks seen during training. Moreover, conventional training pipelines terminate learning after convergence, lacking the adaptability to counter evolving adversarial threats encountered during real-world deployment. This project addresses these limitations in the context of object detection for autonomous vehicle perception systems, focusing on CNN-based detectors such as YOLOv4. We propose a self-supervised continual learning pipeline that enables the detector to adapt and improve during deployment by leveraging adversarial inputs in real time. Our input dataset consists of both clean and adversarial examples generated using Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). We incorporate the MagNet detector to identify adversarial samples during inference, which are then used for training the target YOLOv4 via self-supervised learning mechanism. This mechanism allows the YOLOv4 model to incrementally enhance its robustness through continuous self-supervised updates. Through this adaptive training pipeline, we demonstrate improved resilience of YOLOv4 to adversarial perturbations, achieving a measurable increase in mean Average Precision (mAP) under FGSM and PGD attacks.</p>`;
          g.appendChild(card);
        } else if(id==='kinova-rl') {
          const card=document.createElement('div');
          card.className='vid';
          card.innerHTML = `
            <iframe src="https://www.youtube.com/embed/vhEqvj8y0g4" title="YouTube video" allowfullscreen loading="lazy"></iframe>
            <div class="caption">Kinova Gen3 Lite: RL policy (trained in my modified environment) performing target-reaching control.</div>
            <div class="caption" style="border-top:none">
              <p style="margin:0 0 8px;font-size:18px;color:var(--muted)">
                Checkout the slides for full results and more graphics.
              </p>
              <h3 style="margin-top:0">Project Summary</h3>

              <h4>Objective</h4>
              <p>Build a fully redesigned RL environment for the Kinova Gen3 Lite robotic arm in PyBullet, enabling true continuous-control learning instead of IK-driven motion.</p>

              <h4>Reference Baseline</h4>
              <p>The redesign is based on a critical analysis of the original UR5 environment: <a href="https://github.com/leesweqq/ur5_reinforcement_learning_grasp_object/blob/main/ur5_env.py" target="_blank" rel="noreferrer">UR5 RL Environment</a>.</p>

              <h4>Problems With the Original UR5 Environment</h4>
              <ul>
                <li>Used inverse kinematics (IK) inside the control loop, so the agent did not actually learn robot motion — IK solved most of the task.</li>
                <li>Provided non-Markovian observations (e.g., missing joint states, velocities, and end-effector pose).</li>
                <li>Defined an action space that bypassed robot dynamics by directly commanding end-effector positions.</li>
                <li>Rewards primarily reflected IK convergence rather than meaningful control behavior learned by the policy.</li>
                <li>As a result, it was not suitable for genuine RL-based control learning.</li>
              </ul>

              <h4>Key Improvements in the Redesigned Environment</h4>
              <h5>Joint-Space Continuous Control</h5>
              <ul>
                <li>The agent outputs 6D joint-angle increments (Δq) at each time step.</li>
                <li>These increments are applied directly to the robot under joint limits.</li>
                <li>There is no IK or motion planner inside the RL loop — only low-level position control.</li>
              </ul>

              <h5>18-Dimensional Markov Observation Space</h5>
              <ul>
                <li>Joint angles: 6</li>
                <li>Joint velocities: 6</li>
                <li>End-effector position: 3</li>
                <li>Relative goal vector (goal − end-effector): 3</li>
                <li>This provides full access to both robot dynamics and task geometry, making the environment Markovian.</li>
              </ul>

              <h4>Physics-Grounded Reward Function</h4>
              <h5>Distance shaping</h5>
              <ul>
                <li>Separate penalties for far and near distances.</li>
                <li>A deadband near the goal where distance penalties vanish.</li>
              </ul>

              <h5>Safety penalties</h5>
              <ul>
                <li>Collision penalties for self-collision or unsafe proximity.</li>
                <li>Joint-limit proximity penalties to discourage configurations near limits.</li>
              </ul>

              <h5>Smoothness penalties</h5>
              <ul>
                <li>Penalize action magnitude ‖Δq‖².</li>
                <li>Penalize action change ‖Δqₜ − Δqₜ₋₁‖² for jerk reduction.</li>
              </ul>

              <h5>Success incentives</h5>
              <ul>
                <li>A success bonus when the end-effector enters a small threshold around the goal.</li>
                <li>A time-based bonus encouraging faster convergence.</li>
              </ul>

              <p>Together, these components promote accurate, stable, smooth, and safe multi-joint motion.</p>

              <h4>Curriculum Learning Framework</h4>
              <ul>
                <li>Uses a five-stage curriculum to gradually increase difficulty:
                  <ul>
                    <li>Easy, near-center goals.</li>
                    <li>Stages 2–4: expanding coverage over different regions of the workspace.</li>
                    <li>Fully random goals across the reachable workspace.</li>
                  </ul>
                </li>
                <li>Curriculum progression is controlled via episode thresholds such as [50, 100, 150, 200].</li>
                <li>This improves training stability, sample efficiency, and final performance.</li>
              </ul>

              <h4>Outcome</h4>
              <ul>
                <li>The redesigned Kinova Gen3 Lite environment enables true continuous-control RL, where the agent:
                  <ul>
                    <li>Learns coordinated multi-joint motion under realistic dynamics and constraints.</li>
                    <li>No longer relies on IK shortcuts to solve the task.</li>
                  </ul>
                </li>
                <li>It serves as a research-grade benchmark for:
                  <ul>
                    <li>Robotic reaching,</li>
                    <li>Future grasping and manipulation tasks,</li>
                    <li>Multi-modal (e.g., vision + proprioception) policy learning,
                    </li>
                    <li>And sim-to-real transfer studies.</li>
                  </ul>
                </li>
              </ul>
            </div>
          `;
          g.appendChild(card);
        } else {
          const p=document.createElement('p');
          p.className='subtitle';
          p.textContent='No videos yet. Check the repository above for details.';
          g.appendChild(p);
        }
      }
      document.getElementById('backLink').href = location.pathname + '#projects';
    }

    // Wire up details buttons
    document.querySelectorAll('.details').forEach(btn=>{
      btn.addEventListener('click', ()=>{
        const id = btn.getAttribute('data-project');
        history.pushState({project:id}, '', `?project=${id}`);
        showDetail(id);
      });
    });

    // Initial route
    const qs = new URLSearchParams(location.search);
    const projectId = qs.get('project');
    if(projectId){ showDetail(projectId); } else { showHome(); }

    // popstate
    window.addEventListener('popstate', ()=>{
      const id=(new URLSearchParams(location.search)).get('project');
      if(id) showDetail(id); else showHome();
    });

    // Skills: tools/software only (from resume)
    const tools = [
      {name:'ROS 2', slug:'ros'},
      {name:'ROS 1', slug:'ros'},
      {name:'RViz', slug:''},
      {name:'Gazebo', slug:'gazebo'},
      {name:'Docker', slug:'docker'},
      {name:'Git', slug:'git'},
      {name:'Jira', slug:'jira'},
      {name:'WSL2', slug:'windows'},
      {name:'MATLAB', slug:'mathworks'},
      {name:'Simulink', slug:'mathworks'},
      {name:'OpenCV', slug:'opencv'},
      {name:'PyTorch', slug:'pytorch'},
      {name:'TensorFlow', slug:'tensorflow'},
      {name:'LabVIEW', slug:'labview'},
      {name:'CARLA', slug:''},
      {name:'CUDA', slug:'nvidia'},
      {name:'CANoe', slug:''},
      {name:'CANalyzer', slug:''},
      {name:'Vector', slug:''}
    ];

    const skillsGrid = document.getElementById('skillsGrid');
    const icon = (slug)=> slug ? `https://cdn.simpleicons.org/${slug}` : '';

    tools.forEach(t=>{
      const el = document.createElement('div'); el.className='tool';
      const iSrc = icon(t.slug);
      if(iSrc){
        const img = document.createElement('img'); img.src = iSrc; img.alt = `${t.name} logo`;
        img.onerror = ()=>{ img.remove(); el.classList.add('pill'); el.textContent = t.name; };
        el.appendChild(img);
        const span = document.createElement('span'); span.textContent = t.name; el.appendChild(span);
      } else {
        el.className = 'pill'; el.textContent = t.name;
      }
      skillsGrid.appendChild(el);
    });
  </script>
</body>
</html>
